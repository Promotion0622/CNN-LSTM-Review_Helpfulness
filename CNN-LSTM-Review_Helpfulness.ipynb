{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9647c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AutoModel, AutoTokenizer, TFGPT2Model, Trainer, TrainingArguments\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, mean_squared_error, roc_auc_score, r2_score, roc_curve, auc, mean_absolute_error, confusion_matrix, classification_report)\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (Embedding, Conv1D, LSTM, Dense, Conv2D, GlobalMaxPooling2D, GlobalMaxPooling1D, Reshape)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from gensim.models import Word2Vec\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "os.environ[\"HF_HOME\"] = \"Nampromotion/KoGPT2-Review_Helpfulness\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed816915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # 한글과 공백을 제외한 모든 문자를 제거\n",
    "    return re.sub(\"[^가-힣\\s]\", \"\", text)\n",
    "\n",
    "def train_cnn_lstm():\n",
    "    # 데이터 로드\n",
    "    train_data = pd.read_csv('/home/olga/NSJ/전처리/train.csv')\n",
    "    test_data = pd.read_csv('/home/olga/NSJ/전처리/test.csv')\n",
    "\n",
    "    train_data['review_text'] = train_data['review_text'].apply(preprocess_text)\n",
    "    test_data['review_text'] = test_data['review_text'].apply(preprocess_text)\n",
    "\n",
    "    # 텍스트 데이터 토큰화\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_data['review_text'])\n",
    "    X_train_tokenized = tokenizer.texts_to_sequences(train_data['review_text'])\n",
    "    X_test_tokenized = tokenizer.texts_to_sequences(test_data['review_text'])\n",
    "\n",
    "    # 단어 인덱스\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # Word2Vec 모델 학습을 위한 토큰화된 문장 준비\n",
    "    sentences = [[word for word in str(document).split()] for document in train_data['review_text']]\n",
    "\n",
    "    # Word2Vec 모델 학습\n",
    "    word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, sg=0, workers=4)\n",
    "    word2vec_model.save(\"word2vec.model\")\n",
    "\n",
    "    # 임베딩 행렬 생성\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            vector = word2vec_model.wv[word]\n",
    "            embedding_matrix[i] = vector\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    # 패딩\n",
    "    X_train_padded = pad_sequences(X_train_tokenized, padding='post')\n",
    "    X_test_padded = pad_sequences(X_test_tokenized, padding='post', maxlen=len(X_train_padded[0]))\n",
    "\n",
    "    # CNN-LSTM 모델 구축\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        LSTM(64),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 얼리스타핑\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # 모델 훈련\n",
    "    history = model.fit(X_train_padded, train_data['review_usefulness'], epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    # 모델 평가\n",
    "    y_pred = model.predict(X_test_padded)\n",
    "    y_pred_class = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "    # 학습 과정에서의 loss 및 accuracy 시각화\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Loss 시각화\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Evolution')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy 시각화\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Evolution')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 성능 지표\n",
    "    acc = accuracy_score(test_data['review_usefulness'], y_pred_class)\n",
    "    f1 = f1_score(test_data['review_usefulness'], y_pred_class)\n",
    "    precision = precision_score(test_data['review_usefulness'], y_pred_class)\n",
    "    recall = recall_score(test_data['review_usefulness'], y_pred_class)\n",
    "    roc_auc = roc_auc_score(test_data['review_usefulness'], y_pred)\n",
    "    mse = mean_squared_error(test_data['review_usefulness'], y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(test_data['review_usefulness'], y_pred)\n",
    "    r2 = r2_score(test_data['review_usefulness'], y_pred)\n",
    "\n",
    "    print('Accuracy:', acc)\n",
    "    print('F1 Score:', f1)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('ROC AUC:', roc_auc)\n",
    "    print('Mean Squared Error:', mse)\n",
    "    print('Root Mean Squared Error:', rmse)\n",
    "    print('Mean Absolute Error:', mae)\n",
    "    print('R2 Score:', r2)\n",
    "\n",
    "    return test_data['review_usefulness'], y_pred\n",
    "\n",
    "#y_test_cnn_lstm, y_pred_cnn_lstm = train_cnn_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be50802",
   "metadata": {},
   "source": [
    "# Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train_data = pd.read_csv('/home/olga/NSJ/Yelp/train.csv')\n",
    "test_data = pd.read_csv('/home/olga/NSJ/Yelp/test.csv')\n",
    "\n",
    "# 텍스트 데이터 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train_tokenized = tokenizer.texts_to_sequences(train_data['text'])\n",
    "X_test_tokenized = tokenizer.texts_to_sequences(test_data['text'])\n",
    "\n",
    "# 단어 인덱스\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Word2Vec 모델 학습을 위한 토큰화된 문장 준비\n",
    "sentences = [[word for word in str(document).split()] for document in train_data['text']]\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, sg=0, workers=4)\n",
    "word2vec_model.save(\"word2vec.model\")\n",
    "\n",
    "# 임베딩 행렬 생성\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        vector = word2vec_model.wv[word]\n",
    "        embedding_matrix[i] = vector\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# 패딩\n",
    "X_train_padded = pad_sequences(X_train_tokenized, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_tokenized, padding='post', maxlen=len(X_train_padded[0]))\n",
    "\n",
    "# CNN-LSTM 모델 구축\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 얼리스타핑\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(X_train_padded, train_data['useful'], epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_class = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# 성능 지표\n",
    "acc = accuracy_score(test_data['useful'], y_pred_class)\n",
    "f1 = f1_score(test_data['useful'], y_pred_class)\n",
    "precision = precision_score(test_data['useful'], y_pred_class)\n",
    "recall = recall_score(test_data['useful'], y_pred_class)\n",
    "roc_auc = roc_auc_score(test_data['useful'], y_pred)\n",
    "mse = mean_squared_error(test_data['useful'], y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(test_data['useful'], y_pred)\n",
    "r2 = r2_score(test_data['useful'], y_pred)\n",
    "\n",
    "print('Accuracy:', acc)\n",
    "print('F1 Score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('ROC AUC:', roc_auc)\n",
    "print('Mean Squared Error:', mse)\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('R2 Score:', r2)\n",
    "\n",
    "# 에폭당 정확도 및 로스 그래프 그리기\n",
    "plt.figure(figsize=[8, 6])\n",
    "plt.plot(history.history['accuracy'], 'r', linewidth=3.0)\n",
    "plt.plot(history.history['val_accuracy'], 'b', linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.title('Accuracy Curves', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[8, 6])\n",
    "plt.plot(history.history['loss'], 'r', linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'], 'b', linewidth=3.0)\n",
    "plt.legend(['Training Loss', 'Validation Loss'], fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Loss Curves', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f49f27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
